{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring reconstruction data\n",
    "\n",
    "Some notes.\n",
    "\n",
    "1. Response times are not correct on stimulus/submission records and will need to be reconstructed\n",
    "2. I find *no indication* that there are errors in the final submission. This leaves the weird indicator observations at position 0 to be explained, but it's clear they are not causing the board representation to be incorrect.\n",
    "\n",
    "Some todos.\n",
    "\n",
    "1. Count neighboring pieces at each position for each and both colors for error prediction\n",
    "2. Look at distribution of errors by unique position.\n",
    "3. Should probably do a more proper factor analysis rather than independent tests and regressions, but these are adequate (and clear!) enough for a first pass\n",
    "4. **SUPER IMPORTANT** import real/fake records for all positions!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate\n",
    "\n",
    "Imports and data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as sts\n",
    "import seaborn as sns\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "from lib.utility_functions import *\n",
    "from lib.exp4 import *\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = os.path.expanduser('/Volumes/GoogleDrive/My Drive/Bas Zahy Gianni - Games/Data/4_rcn/Raw Data')\n",
    "data_dir = '../etc/4 Reconstruction/Raw Data/'\n",
    "trained_files_dir = os.path.join(data_dir, 'Trained')\n",
    "naive_files_dir = os.path.join(data_dir, 'Untrained')\n",
    "\n",
    "all_files = get_all_csvs(trained_files_dir) + get_all_csvs(naive_files_dir)\n",
    "\n",
    "DF = load_data(all_files)\n",
    "DF = process_data(DF)\n",
    "\n",
    "bpi, wpi, bpf, wpf = unpack_positions(DF)\n",
    "\n",
    "black_errors = (bpf != bpi).astype(int)\n",
    "white_errors = (wpf != wpi).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_file(all_files[0])\n",
    "initial_map = DF[['Subject ID', 'Initials', 'Condition']]\n",
    "initial_map = initial_map.pivot_table(index='Initials', values=['Subject ID', 'Condition'], aggfunc=lambda x: x.values[0])\n",
    "initial_map.to_csv('../etc/4 Reconstruction/subject_map.csv')\n",
    "# Get initials from file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ex = pd.read_csv(all_files[0], names=[\n",
    "#         'Index', 'Subject ID', 'Player Color',\n",
    "#         'Game Index', 'Move Index', 'Status',\n",
    "#         'Black Position', 'White Position', 'Action',\n",
    "#         'Response Time', 'Time Stamp',\n",
    "#         'Mouse Timestamps', 'Mouse Position'\n",
    "#     ])\n",
    "\n",
    "# no_eyecal = ex['Status'] != 'eyecal'\n",
    "# reconi = ex['Status'] == 'reconi'\n",
    "# reconf = ex['Status'] == 'reconf'\n",
    "# only_ends = reconi | reconf\n",
    "# print(len(ex.loc[reconi]), len(ex.loc[reconf]))\n",
    "# ex.loc[reconi, 'Response Time'] = ex.loc[reconf, 'Time Stamp'].values - ex.loc[reconi, 'Time Stamp'].values\n",
    "# ex.loc[only_ends]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_final_pieces(row):\n",
    "    num_bp = np.sum([int(i) for i in row['Black Position (final)']])\n",
    "    num_wp = np.sum([int(i) for i in row['White Position (final)']])\n",
    "    return num_bp + num_wp\n",
    "\n",
    "DF['Num Pieces (final)'] = DF.apply(count_final_pieces, axis=1)\n",
    "DF['Numerosity Error'] = np.abs(DF['Num Pieces'] - DF['Num Pieces (final)'])\n",
    "DF['Response Time'] = DF['Response Time'] / 1000\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.to_csv('./tidy_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute errors\n",
    "\n",
    "First extract positions as numpy arrays for easier manipulation\n",
    "\n",
    "### Error types\n",
    "\n",
    "\n",
    "- **Black**: differences in black boards\n",
    "- **White**: differences in white boards\n",
    "- **Type I**: \"false positive\"; putting a piece where there was not one\n",
    "- **Type II**: \"false negative\"; neglecting a piece where there should have been one\n",
    "- **Type III**: \"swap\"; switching the color on a piece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are there errors in the board representation data?\n",
    "\n",
    "Yunqi previously had trouble with some oddities in the board construction sequences where a piece would be placed in position 0 (top left corner), but the board representation didn't change.\n",
    "\n",
    "**Answer**: \n",
    "- No sign that there are excessive errors at position 0; not sure what Yunqi did before...\n",
    "- Does *not* explain the quesitionable records. They must be utility indicators for the server/client, but I haven't found where in the code they're being produced or why they're necessary. Something to follow up on, but not a practical problem; those records can simply be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_errors_by_location = black_errors.sum(axis=1)\n",
    "white_errors_by_location = white_errors.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5), squeeze=False, sharex=True, sharey=True)\n",
    "\n",
    "ax = axes[0, 0]\n",
    "ax.bar(np.arange(36), black_errors_by_location)\n",
    "plt.setp(ax, ylabel='Error Count', xlabel='Board Location (Black)')\n",
    "\n",
    "ax = axes[0, 1]\n",
    "ax.bar(np.arange(36), white_errors_by_location)\n",
    "plt.setp(ax, xlabel='Board Location (White)')\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just another very quick confirmation - there are *never* conflicts between pieces of different color in the final representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((bpf == 1) & (wpf == 1)).astype(int).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are there differences in error rates between experts and non experts?\n",
    "\n",
    "Answer: looks like yes to me, but has a complicated and significant relationship with the number of pieces. What's the correct analysis? Is this an ANOVA sort of thing for frequentists?\n",
    "\n",
    "TODO: ELO rating effect for trained subjects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piv = DF.pivot_table(index='Num Pieces', values='Total Errors', columns='Condition', margins=True, aggfunc=np.mean)\n",
    "piv # just showing off pandas here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10), squeeze=False)\n",
    "\n",
    "tr = DF.loc[DF['Condition'] == 'Trained']\n",
    "na = DF.loc[DF['Condition'] == 'Naive']\n",
    "\n",
    "ax = axes[0, 0]\n",
    "sns.barplot(x='Condition', y='Total Errors', data=DF, ax=ax)\n",
    "plt.setp(ax, ylabel='Mean # Errors')\n",
    "ttest = sts.ttest_ind(tr['Total Errors'], na['Total Errors'])\n",
    "print('Condition independent t-test:\\n', ttest, '\\n\\n')\n",
    "\n",
    "ax = axes[0, 1]\n",
    "sns.barplot(x='Num Pieces', y='Total Errors', hue='Condition', data=DF, ax=ax, ci=None)\n",
    "plt.setp(ax, xlabel='Target # Pieces', ylabel='Mean # Errors')\n",
    "\n",
    "ax = axes[1, 1]\n",
    "\n",
    "ax.scatter(tr['Num Pieces'], tr['Total Errors'], alpha=.5)\n",
    "ax.scatter(na['Num Pieces'], na['Total Errors'], alpha=.5)\n",
    "lr = sts.linregress(DF['Num Pieces'], DF['Total Errors'])\n",
    "lr_tr = sts.linregress(tr['Num Pieces'], tr['Total Errors'])\n",
    "lr_na = sts.linregress(na['Num Pieces'], na['Total Errors'])\n",
    "print('# Pieces vs Total Errors correlation\\n\\nAll:', lr, '\\n\\nTrained', lr_tr, '\\n\\nNaive', lr_na)\n",
    "\n",
    "x = np.arange(10, 20)\n",
    "ax.plot(x, x * lr_tr.slope + lr_tr.intercept, linewidth=3)\n",
    "ax.plot(x, x * lr_na.slope + lr_na.intercept, linewidth=3)\n",
    "ax.plot(x, x * lr.slope + lr.intercept, color='black', linewidth=4)\n",
    "plt.setp(ax, ylabel='Total # Errors', xlabel='Target # Pieces')\n",
    "\n",
    "axes[1, 0].set_visible(False)\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are there different patterns for forgetting pieces, adding extras, and switching colors?\n",
    "\n",
    "**Answer**: Yes, looks like it.\n",
    "\n",
    "- More experienced players *may* be *slightly* more likely to add a piece where none previously existed\n",
    "- More experienced players are substantially less likely to forget a piece\n",
    "- More experienced players are somewhat less likely to get the color of a piece wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(21, 5), squeeze=False, sharey=True)\n",
    "\n",
    "ax = axes[0, 0]\n",
    "sns.barplot(x='Condition', y='Type I Errors', data=DF, ax=ax)\n",
    "ttest = sts.ttest_ind(tr['Type I Errors'], na['Type I Errors'])\n",
    "print('Type I Ttest:\\n', ttest, '\\n')\n",
    "\n",
    "ax = axes[0, 1]\n",
    "sns.barplot(x='Condition', y='Type II Errors', data=DF, ax=ax)\n",
    "ttest = sts.ttest_ind(tr['Type II Errors'], na['Type II Errors'])\n",
    "print('Type II Ttest:\\n', ttest, '\\n')\n",
    "\n",
    "ax = axes[0, 2]\n",
    "sns.barplot(x='Condition', y='Type III Errors', data=DF, ax=ax)\n",
    "ttest = sts.ttest_ind(tr['Type III Errors'], na['Type III Errors'])\n",
    "print('Type III Ttest:\\n', ttest, '\\n')\n",
    "\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does error likelihood depend on location?\n",
    "\n",
    "Bonferonni correction!!\n",
    "\n",
    "The below could use some further clarity - looks like it has to do with distribution of stimuli as much as anything else. For example, could divide by number of times a piece is located at that position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(21, 12), squeeze=False)\n",
    "heatmap_kws = {'cbar': False, 'square': True}\n",
    "heatmap = lambda data, ax: sns.heatmap(data.reshape([4, 9]), ax=ax, **heatmap_kws)\n",
    "\n",
    "heatmap(black_errors_by_location, axes[0, 0])\n",
    "heatmap(bpi.sum(axis=1), axes[1, 0])\n",
    "heatmap(black_errors_by_location / bpi.sum(axis=1), axes[2, 0])\n",
    "heatmap(white_errors_by_location, axes[0, 1])\n",
    "heatmap(wpi.sum(axis=1), axes[1, 1])\n",
    "heatmap(white_errors_by_location / wpi.sum(axis=1), axes[2, 1])\n",
    "heatmap(black_errors_by_location + white_errors_by_location, axes[0, 2])\n",
    "heatmap((bpi + wpi).sum(axis=1), axes[1, 2])\n",
    "heatmap((black_errors_by_location + white_errors_by_location) / (bpi + wpi).sum(axis=1), axes[2, 2])\n",
    "\n",
    "plt.setp(axes, yticklabels=[], xticklabels=[])\n",
    "plt.setp(axes[0, 0], ylabel='# Errors', xlabel='Black')\n",
    "plt.setp(axes[0, 1], xlabel='White')\n",
    "plt.setp(axes[0, 2], xlabel='All')\n",
    "plt.setp(axes[1, 0], ylabel='# Occurrences')\n",
    "plt.setp(axes[2, 0], ylabel='# Errors / # Occurrences')\n",
    "\n",
    "sns.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there an effect from real vs fake positions? Is there an interaction between position type and condition?\n",
    "\n",
    "**Answer**: Yes effect; no interaction. It looks like the fake positions were substantially harder for *both* groups. This signals a substantial difference in basic statistics with the real and fake positions. (I think this is probably a manipulation failure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.pivot_table(index='Is Real', columns='Condition', values='Total Errors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANOVA(object):\n",
    "    \"\"\"2-way ANOVA wrapper for a balanced between-withins design\"\"\"\n",
    "    def __init__(self, dataframe, between_factor, within_factor, subject_factor, target, alpha=.05):\n",
    "        self.dataframe = dataframe\n",
    "        self.between_factor = between_factor\n",
    "        self.within_factor = within_factor\n",
    "        self.subject_factor = subject_factor\n",
    "        self.target = target\n",
    "        \n",
    "        self.num_subjects = dataframe[subject_factor].unique().size // 2\n",
    "        self.num_between_levels = dataframe[between_factor].unique().size\n",
    "        self.num_within_levels = dataframe[within_factor].unique().size\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.get_means_table()\n",
    "        \n",
    "        self.subject_pivot = self.means_table.pivot_table(\n",
    "            index=between_factor, values=['False', 'True'], # fix to get values from df\n",
    "            aggfunc=np.mean\n",
    "        )\n",
    "        \n",
    "        self.mu = self.subject_pivot.mean(axis=0).mean()\n",
    "        \n",
    "    def get_means_table(self):\n",
    "        self.means_table = self.dataframe.pivot_table(\n",
    "            index=self.subject_factor,\n",
    "            columns=[self.within_factor],\n",
    "            values=self.target,\n",
    "            aggfunc=np.mean\n",
    "        )\n",
    "        \n",
    "        self.means_table[self.between_factor] = self.means_table.index.map(\n",
    "            self._add_condition_map_func\n",
    "        )\n",
    "        \n",
    "        self.means_table[self.between_factor] = self.means_table[self.between_factor].map(\n",
    "            {'Trained': 0, 'Naive': 1}\n",
    "        )\n",
    "        \n",
    "        self.means_table[self.subject_factor] = self.means_table.index\n",
    "        self.means_table.columns = self.means_table.columns.map(str)\n",
    "        self.means_table_vals = self.means_table.loc[:, ['False', 'True']]\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _add_condition_map_func(self, x):\n",
    "        condition_filter = self.dataframe[self.subject_factor] == x\n",
    "        return self.dataframe.loc[condition_filter, self.between_factor].values[0]\n",
    "        \n",
    "    def df_between(self):\n",
    "        return self.num_between_levels - 1\n",
    "    \n",
    "    def df_within(self):\n",
    "        return self.num_within_levels - 1\n",
    "    \n",
    "    def df_interaction(self):\n",
    "        return self.df_between() * self.df_within()\n",
    "    \n",
    "    def df_subjects(self):\n",
    "        return self.num_between_levels * (self.num_subjects - 1)\n",
    "    \n",
    "    def df_error(self):\n",
    "        return self.df_subjects() * self.df_within()\n",
    "    \n",
    "    def df_total(self):\n",
    "        return self.num_between_levels * self.num_within_levels * self.num_subjects - 1\n",
    "        \n",
    "    def sum_of_squares_between(self):\n",
    "        print(self.subject_pivot.mean(axis=1))\n",
    "        y0 = self.subject_pivot.mean(axis=1).values\n",
    "        y1 = self.mu\n",
    "        \n",
    "        squares = (y0 - y1)**2\n",
    "        ss = squares.sum()\n",
    "        \n",
    "        return self.num_within_levels * self.num_subjects * ss\n",
    "        \n",
    "    def sum_of_squares_within(self):\n",
    "        print(self.subject_pivot.mean(axis=0))\n",
    "        y0 = self.subject_pivot.mean(axis=0).values\n",
    "        y1 = self.mu\n",
    "        \n",
    "        squares = (y0 - y1)**2\n",
    "        ss = squares.sum()\n",
    "        \n",
    "        return self.num_between_levels * self.num_subjects * ss\n",
    "    \n",
    "    def sum_of_squares_interaction(self):\n",
    "        y0 = self.subject_pivot.values\n",
    "        y1 = y0.mean(axis=1)[:, np.newaxis]\n",
    "        y2 = y0.mean(axis=0)[np.newaxis, :]\n",
    "        y3 = self.mu\n",
    "\n",
    "        squares = (y0 - y1 - y2 + y3)**2\n",
    "        ss = squares.sum()\n",
    "        \n",
    "        return self.num_subjects * ss\n",
    "        \n",
    "    def sum_of_squares_subjects(self):\n",
    "        y0 = self.means_table_vals.mean(axis=1).values\n",
    "        y1 = self.subject_pivot.mean(axis=1).values\n",
    "        \n",
    "        y1_p = np.concatenate([\n",
    "            np.tile(y1[0], self.num_subjects),\n",
    "            np.tile(y1[1], self.num_subjects)\n",
    "        ])\n",
    "        \n",
    "        squares = (y0 - y1_p)**2\n",
    "        ss = squares.sum()\n",
    "        \n",
    "        return self.num_within_levels * ss\n",
    "        \n",
    "    def sum_of_squares_error(self):\n",
    "        y0 = self.means_table_vals.values\n",
    "        y1 = self.subject_pivot.values\n",
    "        y2 = self.means_table_vals.mean(axis=1).values\n",
    "        y3 = self.subject_pivot.mean(axis=1).values\n",
    "\n",
    "        y1_p = np.stack([\n",
    "            np.tile(y1[:, 0], self.num_subjects), \n",
    "            np.tile(y1[:, 1], self.num_subjects)\n",
    "        ], axis=1)\n",
    "        \n",
    "        y3_p = np.concatenate([\n",
    "            np.tile(y3[0], self.num_subjects),\n",
    "            np.tile(y3[1], self.num_subjects)\n",
    "        ])\n",
    "        \n",
    "        squares = (y0 - y1_p - y2[:, np.newaxis] + y3_p[:, np.newaxis])**2\n",
    "        ss = squares.sum()\n",
    "        \n",
    "        return ss\n",
    "        \n",
    "    def sum_of_squares_total(self):\n",
    "        y0 = self.means_table_vals.values\n",
    "        y1 = self.mu\n",
    "        \n",
    "        squares = (y0 - y1)**2\n",
    "        ss = squares.sum()\n",
    "        \n",
    "        return ss\n",
    "        \n",
    "    def F_between(self):\n",
    "        ms_between = self.sum_of_squares_between() / self.df_between()\n",
    "        ms_subjects = self.sum_of_squares_subjects() / self.df_subjects()\n",
    "        \n",
    "        F = ms_between / ms_subjects\n",
    "        f_dist = sts.f(self.df_between(), self.df_subjects())\n",
    "        \n",
    "        p_val = f_dist.cdf(F)\n",
    "        \n",
    "        return F, p_val\n",
    "        \n",
    "    def F_within(self):\n",
    "        print(self.sum_of_squares_within())\n",
    "        ms_within = self.sum_of_squares_within() / self.df_within()\n",
    "        ms_error = self.sum_of_squares_error() / self.df_error()\n",
    "        \n",
    "        F = ms_within / ms_error\n",
    "        f_dist = sts.f(self.df_within(), self.df_error())\n",
    "        \n",
    "        p_val = f_dist.cdf(F)\n",
    "        \n",
    "        return F, p_val\n",
    "    \n",
    "    def F_interaction(self):\n",
    "        ms_interaction = self.sum_of_squares_interaction() / self.df_interaction()\n",
    "        ms_error = self.sum_of_squares_error() / self.df_error()\n",
    "        \n",
    "        F = ms_interaction / ms_error\n",
    "        f_dist = sts.f(self.df_interaction(), self.df_error())\n",
    "        \n",
    "        p_val = f_dist.cdf(F)\n",
    "        \n",
    "        return F, p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova = ANOVA(DF, 'Condition', 'Is Real', 'Subject ID', 'Response Time')\n",
    "print(anova.num_subjects, anova.num_between_levels, anova.num_within_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Between\", anova.F_between())\n",
    "print(\"Within\", anova.F_within())\n",
    "print(\"Interaction\", anova.F_interaction())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova.sum_of_squares_subjects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova.sum_of_squares_between()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova.sum_of_squares_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova.sum_of_squares_within()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = anova.means_table.pivot_table(index='Condition', values=['False', 'True'], aggfunc=np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.mean(axis=0).values[np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.values - c.mean(axis=0).values[np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova.means_table_vals.mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(4).reshape([2, 2])\n",
    "b = np.array([1, 0])[:, np.newaxis]\n",
    "c = np.array([1, 0])[np.newaxis, :]\n",
    "\n",
    "a - c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova.means_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_for_weiji = DF.pivot_table(index='Subject ID', columns=['Is Real'], values='Total Errors', aggfunc=np.mean)\n",
    "means_for_weiji['Condition'] = means_for_weiji.index.map(lambda x: DF.loc[DF['Subject ID'] == x, 'Condition'].values[0])\n",
    "means_for_weiji['Condition'] = means_for_weiji['Condition'].map({'Trained': 0, 'Naive': 1})\n",
    "means_for_weiji.to_csv('~/Downloads/reconstruction_means.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_for_weiji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = DF['Total Errors'].mean()\n",
    "Apiv = DF.pivot_table(index='Condition', values='Total Errors')\n",
    "Bpiv = DF.pivot_table(index='Is Real', values='Total Errors')\n",
    "ABpiv = DF.pivot_table(index='Condition', columns='Is Real', values='Total Errors')\n",
    "Sapiv = DF.pivot_table(index='Subject ID', values='Total Errors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sapiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Apiv.loc['Naive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABpiv.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Apiv.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2     # Condition\n",
    "b = 2     # Is Real\n",
    "s = 38    # Subject ID\n",
    "\n",
    "SSA = b * s * ((Apiv - mu)**2).sum()\n",
    "SSB = a * s * ((Bpiv - mu)**2).sum()\n",
    "SSAB = s * ((ABpiv.values - Apiv.values.T - Bpiv.values + mu)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = Sapiv.copy()\n",
    "term.loc[means_for_weiji['Condition'] == 0] = term.loc[means_for_weiji['Condition'] == 0].values - Apiv.loc['Trained'].values\n",
    "term.loc[means_for_weiji['Condition'] == 1] = term.loc[means_for_weiji['Condition'] == 1].values - Apiv.loc['Naive'].values\n",
    "SSSa = b * (term.values**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = means_for_weiji.copy()\n",
    "def calculate_term(row):\n",
    "    \n",
    "\n",
    "\n",
    "term.loc[term['Condition'] == 0, [False, True]] = term.loc[term['Condition'] == 0, [False, True]].values - ABpiv.loc['Trained'].values + Apiv.loc['Trained'].values\n",
    "term.loc[term['Condition'] == 1, ['False', 'True']] = term.loc[term['Condition'] == 1, ['False', 'True']].values - ABpiv.loc['Naive'].values + Apiv.loc['Naive'].values\n",
    "term.loc[:] = term.values - Sapiv.values\n",
    "SSB_Sa = (term.values**2).sum*()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(means_for_weiji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smDF = pd.DataFrame(index=DF.index, columns=['target', 'f1', 'f2'])\n",
    "\n",
    "smDF['target'] = DF['Total Errors']\n",
    "smDF['f3'] = DF['Subject ID']\n",
    "smDF['f2'] = DF['Is Real']\n",
    "smDF['f1'] = DF['Condition']\n",
    "\n",
    "formula = 'target ~ C(f1) + C(f2):C(f3) + C(f1):C(f2):C(f3)'\n",
    "model = ols(formula, smDF).fit()\n",
    "\n",
    "anova_table = statsmodels.stats.anova.anova_lm(model, typ=1)\n",
    "anova_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smDF = pd.DataFrame(index=DF.index, columns=['target', 'f1', 'f2'])\n",
    "\n",
    "smDF['target'] = DF['Numerosity Error']\n",
    "smDF['f2'] = DF['Is Real']\n",
    "smDF['f1'] = DF['Condition']\n",
    "\n",
    "formula = 'target ~ C(f1) + C(f2) + C(f1):C(f2)'\n",
    "model = ols(formula, smDF).fit()\n",
    "\n",
    "anova_table = statsmodels.stats.anova.anova_lm(model, typ=1)\n",
    "anova_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smDF = pd.DataFrame(index=DF.index, columns=['target', 'f1', 'f2'])\n",
    "\n",
    "smDF['target'] = DF['Total Errors']\n",
    "smDF['f2'] = DF['Is Real']\n",
    "smDF['f1'] = DF['Condition']\n",
    "smDF['f3'] = DF['Num Pieces']\n",
    "\n",
    "\n",
    "formula = 'target ~ C(f1) + C(f2) + C(f3) + C(f1):C(f2) + C(f2):C(f3) + C(f1):C(f3) + C(f1):C(f2):C(f3)'\n",
    "model = ols(formula, smDF).fit()\n",
    "\n",
    "anova_table = statsmodels.stats.anova.anova_lm(model, typ=1)\n",
    "anova_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smDF = pd.DataFrame(index=DF.index, columns=['target', 'f1', 'f2'])\n",
    "\n",
    "smDF['target'] = DF['Numerosity Error']\n",
    "smDF['f2'] = DF['Is Real']\n",
    "smDF['f1'] = DF['Condition']\n",
    "smDF['f3'] = DF['Num Pieces']\n",
    "\n",
    "\n",
    "formula = 'target ~ C(f1) + C(f2) + C(f3) + C(f1):C(f2) + C(f2):C(f3) + C(f1):C(f3) + C(f1):C(f2):C(f3)'\n",
    "model = ols(formula, smDF).fit()\n",
    "\n",
    "anova_table = statsmodels.stats.anova.anova_lm(model, typ=1)\n",
    "anova_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.barplot(x='Is Real', y='Total Errors', hue='Condition', data=DF)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.barplot(x='Condition', y='Numerosity Error', hue='Is Real', data=DF)\n",
    "ax = plt.gca()\n",
    "ax.legend(loc=0)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.plot(x='Position ID', y='Total Errors', hue='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.Condition.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pidpiv = DF.pivot_table(index='Position ID', values=['Total Errors', 'Num Pieces'], columns='Condition')\n",
    "pidpiv.sort_values(['Num Pieces', 'Trained'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pidpiv = pidpiv.sort_values('Trained')\n",
    "plt.plot(pidpiv['Trained'].values, label='Trained')\n",
    "plt.plot(pidpiv['Trained']['False'].values, label='Naive')\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.legend(loc=0)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrap\n",
    "\n",
    "Make this into a standalone script at some point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../etc/4 Reconstruction/stimuli.txt', mode='r') as f:\n",
    "    positions = f.readlines()\n",
    "    \n",
    "def strip_position(position_string):\n",
    "    s = [p.strip(\"'\") for p in position_string.split('(')[1].split(')')[0].split(', ')[:2]]\n",
    "    return s[0] + s[1]\n",
    "\n",
    "stimuli = list(map(strip_position, positions))\n",
    "fake_mask = np.ones(len(stimuli), dtype=int)\n",
    "fake_mask[:len(stimuli)//2] = 0\n",
    "\n",
    "stim_map = pd.DataFrame(index=stimuli, data=fake_mask.astype(bool), columns=['Is Real'])\n",
    "stim_map['Position ID'] = np.arange(len(stim_map), dtype=int)\n",
    "stim_map['Position dummy'] = stim_map.index\n",
    "stim_map = stim_map.drop_duplicates(subset='Position dummy')\n",
    "stim_map[['Is Real', 'Position ID']].to_csv('../etc/4 Reconstruction/position_map.csv', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_map = pd.read_csv('../etc/4 Reconstruction/position_map.csv', index_col=0, skiprows=1, names=['Position', 'Is Real', 'Position ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
